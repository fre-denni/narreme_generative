{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":10669844,"sourceType":"datasetVersion","datasetId":6608464},{"sourceId":10735606,"sourceType":"datasetVersion","datasetId":6656527},{"sourceId":10736309,"sourceType":"datasetVersion","datasetId":6657027}],"dockerImageVersionId":30839,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Narreme Visualization\n\nThis second part of the code will focus on enphasizing the visualization and exploration of the different documents and topics we extracted from the previous notebook. By doing those we can understand the linking between each documents so that to create narrative connections between them.\n\nThis notebook is part of a master's thesis project in Digital Interaction Design at Politecnico di Milano, by Federico Denni.","metadata":{}},{"cell_type":"code","source":"from IPython.display import clear_output\n!pip install pyvis networkx seaborn spacy\n!python -m spacy download xx_sent_ud_sm\nclear_output()","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nimport spacy #nlp\nfrom spacy import displacy #spacy visualizer library\nimport pyvis #interactive visualization\nimport networkx as nx\nfrom pyvis.network import Network\nimport matplotlib.pyplot as plt #plots\nimport seaborn as sns #make these plots nice\n\nplt.style.use('ggplot')\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"try:\n    #Change this directory for other dataset\n    df = pd.read_csv(r'/kaggle/input/probes-cultural/probes_analysis.csv',engine='python', encoding=\"utf-8\")\n    #eliminate compound scores\n    \nexcept FileNotFoundError:\n      print(\"Error: file not found. Please upload the file or provide the correct path.\")","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.080Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"df.head(10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.080Z"}},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"---\n\nNow that is loaded, lets proceed to analyze and clean the different elements","metadata":{}},{"cell_type":"code","source":"#let's split the docs_n in sentences\nnlp = spacy.load(\"xx_sent_ud_sm\")#translate all sentences in english before using this\n\n# Define a function to tokenize text using spaCy\ndef tokenize(text):\n    if isinstance(text, str):\n        doc = nlp(text)\n        return [token.text for token in doc]\n    return []\n\n# Define a function to split text into sentences using spaCy\ndef split_sentences(text):\n    if isinstance(text, str):\n        doc = nlp(text)\n        return [sent.text for sent in doc.sents]\n    return []\n\n# Apply the sentence splitting function to the new columns\ndf['docs_1_sentences'] = df['docs_1'].apply(split_sentences)\ndf['docs_2_sentences'] = df['docs_2'].apply(split_sentences)\ndf['docs_3_sentences'] = df['docs_3'].apply(split_sentences)\n\ndf.to_csv(r'/kaggle/working/sentences.csv', index=False)\n\ndf.head(10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#lets now identify and extract the entities type in the sentences\nnlp = spacy.load(\"en_core_web_sm\")#translate all sentences in english before using this\n\ndf = pd.read_csv(r'/kaggle/working/sentences.csv', encoding=\"utf-8\")\ncolumns_to_extract = ['docs_1_sentences', 'docs_2_sentences', 'docs_3_sentences']\n\ndef get_entities_and_keywords(sentences, keywords):\n    entities = []\n    for sentence in sentences:\n        doc = nlp(sentence)\n        sentence_entities = [ent.label_ for ent in doc.ents] #if MISC, add word\n        for keyword in keywords:\n            if keyword in sentence:\n                sentence_entities.append(keyword)\n                \n        entities.append(sentence_entities)\n    return entities\n\n# Apply the function to each column\ndf['doc_1_entity_list'] = df.apply(lambda row: get_entities_and_keywords(eval(row['docs_1_sentences']), eval(row['Keywords'])), axis=1)\ndf['doc_2_entity_list'] = df.apply(lambda row: get_entities_and_keywords(eval(row['docs_2_sentences']), eval(row['Keywords'])), axis=1)\ndf['doc_3_entity_list'] = df.apply(lambda row: get_entities_and_keywords(eval(row['docs_3_sentences']), eval(row['Keywords'])), axis=1)\n\ndf.head(10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#transform all the list in a unique one and then eliminate whitespaces.\n#filter out cardinals, percent\ndef extract_unique_words(list_of_lists):\n    unique_words = set()\n    for sublist in list_of_lists:\n        unique_words.update(sublist)\n    return [word for word in unique_words if word not in ['CARDINAL', 'PERCENT']] #we are not interested in those\n\n# Apply the function to the specified columns\nfor column in ['doc_1_entity_list', 'doc_2_entity_list', 'doc_3_entity_list']:\n    df[column] = df[column].apply(extract_unique_words)\n\n\ndf.to_csv(r'/kaggle/working/ent_list.csv', index=False)\ndf.head(10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import ast  # To safely evaluate string representations of lists\ndf = pd.read_csv(r'/kaggle/working/ent_list.csv', encoding=\"utf-8\")\n\n# Convert string representations of lists to actual Python lists\nfor col in ['doc_1_entity_list', 'doc_2_entity_list', 'doc_3_entity_list']:\n    df[col] = df[col].apply(ast.literal_eval)\n\n# Extract entities and matches as previously discussed\ndef extract_entities(df):\n    entity_data = []\n    for _, row in df.iterrows():\n        topic_id = row['Topic']\n        for doc_col in ['doc_1_entity_list', 'doc_2_entity_list', 'doc_3_entity_list']:\n            doc_name = doc_col.split('_')[1]  # Extract doc name (e.g., '1', '2', '3')\n            entity_list = row[doc_col]\n            if isinstance(entity_list, list):  # Ensure it's a list\n                for word in entity_list:\n                    entity_data.append({\n                        'word': word,\n                        'topic': topic_id,\n                        'doc': f'{doc_name}_topic_{topic_id}',\n                        'text': row[f'docs_{doc_name}']\n                    })\n            else:\n                print(f\"Warning: {doc_col} is not a list for row {row.name}\") #Check this row, something happened\n    return pd.DataFrame(entity_data)\n\n# Create a DataFrame with all entities, their topics, and document sources\nentities_df = extract_entities(df)\n\n# Perform a self-join to find matches across all rows and columns\nmatches_df = entities_df.merge(\n    entities_df,\n    on='word',\n    suffixes=('_source', '_target')\n)\n\n# Filter out self-matches (where source and target are identical)\nmatches_df = matches_df[matches_df['doc_source'] != matches_df['doc_target']]\n\n# Eliminate duplicates by ensuring consistent ordering of source and target\nmatches_df['match'] = matches_df.apply(\n    lambda x: tuple(sorted([x['doc_source'], x['doc_target']])), axis=1\n)\n\n# Keep only unique matches\nuniques = matches_df.drop_duplicates(subset='match').drop(columns='match')\n\n# Save the results to a CSV file (optional)\nuniques.to_csv(r'/kaggle/working/matches.csv', index=False)\nuniques.head(100)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"#Make nodes file\n# Load your matches.csv\nmatches_df = pd.read_csv(r'/kaggle/working/matches.csv')\n\n# Extract unique nodes from source and target columns\nsource_nodes = matches_df[['doc_source', 'topic_source', 'text_source']].rename(columns={'doc_source': 'Id', 'topic_source': 'topic','text_source':'text'})\ntarget_nodes = matches_df[['doc_target', 'topic_target', 'text_target']].rename(columns={'doc_target': 'Id', 'topic_target': 'topic','text_target':'text'})\nnodes_df = pd.concat([source_nodes, target_nodes]).drop_duplicates(subset=['Id'])\n\n# Add additional attributes if needed\n# For example, you can extract the document number and topic number from the Id\nnodes_df['document_number'] = nodes_df['Id'].apply(lambda x: int(x.split('_')[0]))\nnodes_df['topic_number'] = nodes_df['Id'].apply(lambda x: int(x.split('_topic_')[1]))\n\n# Save the nodes file\nnodes_df.to_csv('nodes.csv', index=False)\nnodes_df.head(10)\n","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Load your matches.csv\nmatches_df = pd.read_csv(r'/kaggle/working/matches.csv')\n\n# Create the edges DataFrame\nedges_df = matches_df[['doc_source', 'doc_target', 'word']]\nedges_df = edges_df.rename(columns={'doc_source': 'Source', 'doc_target': 'Target', 'word': 'Shared_Word'})\n\n# Calculate edge weights based on shared word occurrences\nedge_weights = edges_df.groupby(['Source', 'Target']).size().reset_index(name='Weight')\n\n# Merge edge weights back into the edges DataFrame\nedges_df = pd.merge(edges_df[['Source', 'Target', 'Shared_Word']], edge_weights, on=['Source', 'Target'], how='left')\n\n# Remove duplicate edges, keeping the weight\nedges_df = edges_df.drop_duplicates(subset=['Source', 'Target'])\n\n# Save the edges file\nedges_df.to_csv('edges.csv', index=False)\n\nedges_df.head(10)","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.081Z"}},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import json\n\ndf = pd.read_csv(r'/kaggle/working/matches.csv', encoding=\"utf-8\")\n\n# Create a graph using networkx\nG = nx.Graph()\n\n# Define a color mapping for topics (6 topics)\ntopic_colors = {\n    0: 'red',\n    1: 'blue',\n    2: 'green',\n    3: 'orange',\n    4: 'purple',\n    5: 'cyan'\n}\n\n# Create a Pyvis network visualization\nnet = Network(\n    notebook=True,\n    cdn_resources=\"remote\",\n    bgcolor=\"#222222\",\n    font_color=\"white\",\n    height=\"750px\",\n    width=\"100%\",\n    select_menu=True,\n    filter_menu=True\n)\n\n# Add nodes with clustering by document number and coloring by topic\nfor _, row in uniques.iterrows():\n    topic_id_source = int(row['doc_source'].split('_topic_')[1])  # Extract topic ID from source doc name\n    topic_id_target = int(row['doc_target'].split('_topic_')[1])  # Extract topic ID from target doc name\n    \n       # Prepare HTML content for hover frame\n    source_text_html = f\"{row['text_source']}\"\n    target_text_html = f\"{row['text_target']}\"\n    \n    # Add source node with color based on topic and HTML content as title\n    net.add_node(\n        row['doc_source'], \n        label=row['doc_source'], \n        color=topic_colors[topic_id_source], \n        title=source_text_html  # Use HTML formatted text for hover\n    )\n    \n    # Add target node with color based on topic and HTML content as title\n    net.add_node(\n        row['doc_target'], \n        label=row['doc_target'], \n        color=topic_colors[topic_id_target], \n        title=target_text_html  # Use HTML formatted text for hover\n    )\n\n# Add nodes with text as data attributes for hover functionality\nnode_connections = {}  # Dictionary to track node degrees\n\nfor _, row in uniques.iterrows():\n    # Increment connection count for source and target nodes\n    node_connections[row['doc_source']] = node_connections.get(row['doc_source'], 0) + 1\n    node_connections[row['doc_target']] = node_connections.get(row['doc_target'], 0) + 1\n\n# Add edges to the graph from the unique matches DataFrame\nfor _, row in uniques.iterrows():\n    net.add_edge(row['doc_source'], row['doc_target'])\n\n# Adjust node sizes based on their number of connections (degree)\nfor node_id, degree in node_connections.items():\n    net.get_node(node_id)['size'] = degree * 0.8  # Scale size; adjust multiplier as needed\n\n# Convert NetworkX graph to Pyvis format\nnet.from_nx(G)\n\n# Set options for smaller arrows and hierarchical layout\noptions = {\n    \"layout\": {\n        \"hierarchical\": {\n            \"enabled\": False,\n            \"levelSeparation\": 150,\n            \"nodeSpacing\": 200,\n            \"treeSpacing\": 200,\n            \"direction\": \"UD\",\n            \"sortMethod\": \"hubsize\"\n        }\n    },\n    \"nodes\": {\n        \"shape\": \"text\",\n        \"font\": {\n            \"size\": 12,\n            \"color\": \"#ffffff\"\n        }\n    },\n    \"edges\": {\n        \"arrows\": {\n            \"to\": {\n                \"enabled\": True,\n                \"scaleFactor\": 0.5, # Smaller arrows\n                \"type\": \"arrow\"\n            }\n        },\n        \"smooth\": False\n    }\n}\n\nnet.set_options(json.dumps(options))\n\n# Save the network visualization to an HTML file\nnet.save_graph('network_visualization.html')\n# Save and show the network visualization\nnet.show('network_visualization.html')","metadata":{"trusted":true,"execution":{"execution_failed":"2025-02-13T01:12:57.081Z"}},"outputs":[],"execution_count":null}]}